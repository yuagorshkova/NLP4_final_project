{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Вам нужно получить доступ к чекпоинту, который хранится у меня на Гугл диске. Нужно сделать следующее (я проверяла, это работает):\n",
        "1. Go to Shared with me in Google Drive.\n",
        "2. Select the folder or file you want to acess.\n",
        "3. Right click on it and choose Add shortcut to drive.\n",
        "4. A pop-up window will apear, Select MyDrive then click on Add Shortcut.\n",
        "\n",
        "Ссылка на папку с чекпоинтом: https://drive.google.com/drive/folders/1mUKfgYDcfzQQglhbTowFtmCFe0lxdgFT?usp=sharing"
      ],
      "metadata": {
        "id": "1TjZ2uq0AKmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#загрузить данные\n",
        "!wget -nc https://github.com/named-entity/hse-nlp/raw/master/4th_year/Project/train_reviews.txt"
      ],
      "metadata": {
        "id": "QsmOPzXu_iOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "Hajclhh2_bHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_data_filepath = \"/content/train_reviews.txt\"\n",
        "out_filename = \"pred_categories.txt\"\n",
        "\n",
        "checkpoint_path = \"/content/gdrive/MyDrive/gorshkova_checkpoints/CategorySentiment.ckpt\""
      ],
      "metadata": {
        "id": "xICJ4qTN_ZPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Это можно просто запустить, оно само сработает и выдаст предсказание в указанный файл"
      ],
      "metadata": {
        "id": "60-H4KeS-Edm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##imports and constants"
      ],
      "metadata": {
        "id": "eRV7uk5K-EzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LABEL_COLUMNS = [\"Food\", \"Interior\", \"Price\", \"Whole\", \"Service\"]\n",
        "id2label = {0: \"absence\",\n",
        "            1: \"negative\", \n",
        "            2: \"neutral\",\n",
        "            3: \"positive\", \n",
        "            4: \"both\"}"
      ],
      "metadata": {
        "id": "6tNgZ2os-jWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3H7rAJfr98eo"
      },
      "outputs": [],
      "source": [
        "!pip install transformers --upgrade --quiet\n",
        "!pip install pytorch_lightning --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from collections import defaultdict\n",
        "import os\n",
        "import torch\n",
        "import torchmetrics\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from transformers import get_polynomial_decay_schedule_with_warmup, get_linear_schedule_with_warmup\n",
        "\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset and Datamodule"
      ],
      "metadata": {
        "id": "LNIv-yo8-Z6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CatSentDataset(Dataset):\n",
        "\n",
        "  def __init__(self, \n",
        "               data: pd.DataFrame, \n",
        "               tokenizer: BertTokenizer,\n",
        "               max_token_len: int = 128):\n",
        "    self.data = data\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_token_len = max_token_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index: int):\n",
        "    item_data = self.data.iloc[index]\n",
        "    text = item_data[\"text\"]\n",
        "    \n",
        "    encoding = self.tokenizer(text, max_length = self.max_token_len,\n",
        "                              padding = \"max_length\", truncation = True, \n",
        "                              return_attention_mask = True, return_tensors = \"pt\")\n",
        "    \n",
        "    labels = item_data[LABEL_COLUMNS]\n",
        "    labels = torch.LongTensor(labels)\n",
        "    #labels = torch.unsqueeze(labels, 1)\n",
        "\n",
        "    return dict(\n",
        "        text=text,\n",
        "        input_ids = encoding[\"input_ids\"].flatten(),\n",
        "        attention_mask = encoding[\"attention_mask\"].flatten(),\n",
        "        labels=labels)\n",
        "\n",
        "class CatSentPredictDataset(Dataset):\n",
        "  def __init__(self, \n",
        "               data: pd.DataFrame, \n",
        "               tokenizer: BertTokenizer,\n",
        "               max_token_len: int = 128):\n",
        "    self.data = data\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_token_len = max_token_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index: int):\n",
        "    item_data = self.data.iloc[index]\n",
        "    text = item_data[\"text\"]\n",
        "    \n",
        "    encoding = self.tokenizer(text, max_length = self.max_token_len,\n",
        "                              padding = \"max_length\", truncation = True, \n",
        "                              return_attention_mask = True, return_tensors = \"pt\")\n",
        "\n",
        "    return dict(\n",
        "        text=text,\n",
        "        input_ids = encoding[\"input_ids\"].flatten(),\n",
        "        attention_mask = encoding[\"attention_mask\"].flatten())"
      ],
      "metadata": {
        "id": "yJMY3nUb-c11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CatSentDataModule(pl.LightningDataModule):\n",
        "  def __init__(self, \n",
        "               data: pd.DataFrame, \n",
        "               predict_data: pd.DataFrame = pd.DataFrame(),\n",
        "               batch_size: int = 32,\n",
        "               eval_fraction = 0.3,\n",
        "               max_token_len: int = 128,\n",
        "               ):\n",
        "    super().__init__()\n",
        "    self.data = data\n",
        "    self.predict_data = predict_data\n",
        "    self.batch_size = batch_size\n",
        "    self.eval_fraction = eval_fraction\n",
        "    self.tokenizer = BertTokenizer.from_pretrained(\"sberbank-ai/ruBert-base\", never_split=[\"USER\"])\n",
        "    self.max_token_len = max_token_len\n",
        "\n",
        "  def setup(self, stage: str):\n",
        "\n",
        "    data_full = CatSentDataset(self.data,\n",
        "                              self.tokenizer,\n",
        "                              self.max_token_len)\n",
        "    eval_len = int(self.data.shape[0] * self.eval_fraction)\n",
        "    train_len = self.data.shape[0] - eval_len\n",
        "    self.data_train, self.data_eval = random_split(data_full, \n",
        "                                         [train_len, eval_len], \n",
        "                                         generator=torch.Generator().manual_seed(1000))\n",
        "\n",
        "\n",
        "  def train_dataloader(self):\n",
        "      return DataLoader(self.data_train, batch_size = self.batch_size,\n",
        "                        shuffle = True, num_workers = 2)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "      return DataLoader(self.data_eval, batch_size = self.batch_size, num_workers = 2)\n",
        "\n",
        "  def predict_dataloader(self):\n",
        "      dataset = CatSentPredictDataset(self.predict_data, self.tokenizer, self.max_token_len)\n",
        "      return DataLoader(dataset, batch_size = self.batch_size, num_workers = 2, shuffle=False)"
      ],
      "metadata": {
        "id": "slNuuSfG-whw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "1x5sldfx-w90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CatSentClassifier(pl.LightningModule):\n",
        "  def __init__(self, \n",
        "               num_labels: int,\n",
        "               num_tasks: int,\n",
        "               learning_rate : float = 2e-5):\n",
        "    super().__init__()\n",
        "\n",
        "    self.save_hyperparameters()\n",
        "\n",
        "    self.num_labels = num_labels #одинаковые лейблы для каждой задачи: [0, 4]\n",
        "    self.num_tasks = num_tasks\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "    self.bert = BertModel.from_pretrained(\"sberbank-ai/ruBert-base\", \n",
        "                                          output_attentions=True,\n",
        "                                          output_hidden_states=True,\n",
        "                                          ) \n",
        "    for param in self.bert.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    self.classifiers = nn.ModuleList([nn.Linear(self.bert.config.hidden_size, self.num_labels) for _ in range(self.num_tasks)]) \n",
        "\n",
        "    self.criterions = nn.ModuleList([nn.CrossEntropyLoss() for _ in range(self.num_tasks)]) \n",
        "\n",
        "    \n",
        "  def forward(self, input_ids, attention_mask, labels=None):\n",
        "    bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    attn = bert_output.attentions\n",
        "    h_cls = bert_output.pooler_output\n",
        "    outputs =  [l(h_cls) for l in self.classifiers]\n",
        "    return torch.stack((outputs), dim=0)\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    input_ids = batch[\"input_ids\"]\n",
        "    attention_mask = batch[\"attention_mask\"]\n",
        "    labels = batch[\"labels\"].T\n",
        "\n",
        "    outputs = self(input_ids, attention_mask)\n",
        "\n",
        "    losses = [loss(output, label) for loss, output, label in zip(self.criterions, outputs, labels)]\n",
        "    total_loss = sum(losses)\n",
        "    self.log(\"train_loss\", total_loss, prog_bar=True, logger=True)\n",
        "    return {\"loss\": total_loss, \"predictions\": outputs.transpose(0,1), \"labels\": labels.transpose(0,1)}\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    input_ids = batch[\"input_ids\"]\n",
        "    attention_mask = batch[\"attention_mask\"]\n",
        "    labels = batch[\"labels\"].T\n",
        "\n",
        "    outputs = self(input_ids, attention_mask)\n",
        "\n",
        "    losses = [loss(output, label) for loss, output, label in zip(self.criterions, outputs, labels)]\n",
        "    total_loss = sum(losses)\n",
        "    self.log(\"val_loss\", total_loss, prog_bar=True, logger=True, sync_dist=True,\n",
        "             on_step=True, on_epoch=True)\n",
        "    return {\"predictions\" : outputs.transpose(0,1), \"labels\" : labels.transpose(0,1)}\n",
        "\n",
        "  def validation_epoch_end(self, val_outputs):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    for ouput_dict in val_outputs:\n",
        "      y_pred.append(ouput_dict[\"predictions\"])\n",
        "      y_true.append(ouput_dict[\"labels\"])\n",
        "    y_pred = torch.cat(y_pred, dim=0).softmax(dim=2).argmax(dim=2).flatten()\n",
        "    y_true = torch.cat(y_true, dim=0).flatten()\n",
        "    accuracy = (torch.eq(y_pred, y_true).sum()/y_true.shape[0]).item()\n",
        "    self.log(\"val_accuracy\", accuracy, logger=True)\n",
        "    \n",
        "\n",
        "  def predict_step(self, batch, batch_idx):\n",
        "    input_ids = batch[\"input_ids\"]\n",
        "    attention_mask = batch[\"attention_mask\"]\n",
        "\n",
        "    outputs = self(input_ids, attention_mask)\n",
        "    outputs = outputs.softmax(dim=2)\n",
        "    return outputs.argmax(dim=2).T\n",
        "\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "\n",
        "    optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
        "    return optimizer\n",
        "\n"
      ],
      "metadata": {
        "id": "bYDLqqDV-zCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## prediction"
      ],
      "metadata": {
        "id": "-KbHTQ5y_Rkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_data = defaultdict(list)\n",
        "with open(pred_data_filepath) as f:\n",
        "  for line in f.readlines():\n",
        "    k,v = line.strip().split(\"\\t\")\n",
        "    pred_data[\"text_id\"].append(k)\n",
        "    pred_data[\"text\"].append(v)\n",
        "pred_data = pd.DataFrame(pred_data)\n",
        "pred_data.head(2)"
      ],
      "metadata": {
        "id": "67zsGr9h_UiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CatSentClassifier.load_from_checkpoint(checkpoint_path)"
      ],
      "metadata": {
        "id": "v72N_H76_fst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_trainer = pl.Trainer()\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "MAX_TOKEN_LEN = 128\n",
        "pred_data_module = CatSentDataModule(pred_data, pred_data,\n",
        "                               batch_size = BATCH_SIZE,\n",
        "                               max_token_len = MAX_TOKEN_LEN)\n",
        "pred_data_module.setup(\"predict\")"
      ],
      "metadata": {
        "id": "LdUqLUwc_nYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = pred_trainer.predict(model, pred_data_module.predict_dataloader())\n",
        "preds = []\n",
        "for p in predictions:\n",
        "  preds.extend(p)\n",
        "preds = torch.stack(preds, dim=0)\n",
        "preds.size()"
      ],
      "metadata": {
        "id": "dKKmQZFf_uZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(out_filename, \"w\") as outf:\n",
        "  for text_id, pred in zip(pred_data.text_id, preds):\n",
        "    for category, label_id in zip(LABEL_COLUMNS, pred):\n",
        "      outf.write(\"\\t\".join([text_id, category, id2label[label_id.item()]]) + \"\\n\")"
      ],
      "metadata": {
        "id": "m7nBvxhk_wDE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}