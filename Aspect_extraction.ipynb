{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install stanza\n",
        "! pip install -U 'scikit-learn<0.24'\n",
        "!pip install sklearn-crfsuite"
      ],
      "metadata": {
        "id": "YZXh71bfJe11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import stanza\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics"
      ],
      "metadata": {
        "id": "ehLLc40U7JHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stanza.download('ru')\n",
        "nlp = stanza.Pipeline('ru', processors='tokenize,pos')"
      ],
      "metadata": {
        "id": "uHQmy4T3Jqcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для BIO-тэггинга последовательностей"
      ],
      "metadata": {
        "id": "9FIrNuvQza3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bio_tagged_df(reviews, aspects):\n",
        "  texts_ids = []\n",
        "  texts = []\n",
        "  pos_tags = []\n",
        "  bio_tags = []\n",
        "  starts = []\n",
        "  stops = []\n",
        "\n",
        "  for text_id, text in tqdm(reviews.items()):\n",
        "  \n",
        "    processed = nlp(text)\n",
        "  \n",
        "    for token in processed.iter_tokens():\n",
        "    \n",
        "      add = False\n",
        "    \n",
        "      for mention in aspects[text_id]:\n",
        "        if token.start_char == int(mention['start']) and token.end_char <= int(mention['stop']):\n",
        "\n",
        "          texts_ids.append(text_id)\n",
        "          texts.append(token.text)\n",
        "          pos_tags.append(token.words[0].upos)\n",
        "          bio_tags.append('B-'+mention['aspect'])\n",
        "          starts.append(token.start_char)\n",
        "          stops.append(token.end_char)\n",
        "          add = True\n",
        "\n",
        "        elif token.start_char > int(mention['start']) and token.end_char <= int(mention['stop']):\n",
        "          texts_ids.append(text_id)\n",
        "          texts.append(token.text)\n",
        "          pos_tags.append(token.words[0].upos)\n",
        "          bio_tags.append('I-'+mention['aspect'])\n",
        "          starts.append(token.start_char)\n",
        "          stops.append(token.end_char)\n",
        "          add = True\n",
        "\n",
        "      if not add:\n",
        "        texts_ids.append(text_id)\n",
        "        texts.append(token.text)\n",
        "        pos_tags.append(token.words[0].upos)\n",
        "        bio_tags.append('O')\n",
        "        starts.append(token.start_char)\n",
        "        stops.append(token.end_char)\n",
        "\n",
        "  bio_df = pd.DataFrame({'text_id': texts_ids,\n",
        "                         'text': texts,\n",
        "                         'pos_tag': pos_tags,\n",
        "                         'bio_tag': bio_tags,\n",
        "                         'start': starts,\n",
        "                         'stop': stops\n",
        "                         })\n",
        "  return bio_df"
      ],
      "metadata": {
        "id": "HJh3uxQEJx0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сразу возьмем все тестовые отзывы (не split), чтобы было больше данных, как тест будем использовать таргетные отзывы"
      ],
      "metadata": {
        "id": "cbX0dCDNzLaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train"
      ],
      "metadata": {
        "id": "7PgSuZAmhien"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tj93tsVX3HOn",
        "outputId": "5d5b5dc9-3d83-4470-e9d0-c556ef08bbf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-28 16:23:50--  https://raw.githubusercontent.com/named-entity/hse-nlp/master/4th_year/Project/train_aspects.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228391 (223K) [text/plain]\n",
            "Saving to: ‘train_aspects.txt’\n",
            "\n",
            "\rtrain_aspects.txt     0%[                    ]       0  --.-KB/s               \rtrain_aspects.txt   100%[===================>] 223.04K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-12-28 16:23:50 (10.4 MB/s) - ‘train_aspects.txt’ saved [228391/228391]\n",
            "\n",
            "--2022-12-28 16:23:50--  https://raw.githubusercontent.com/named-entity/hse-nlp/master/4th_year/Project/train_reviews.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 446118 (436K) [text/plain]\n",
            "Saving to: ‘train_reviews.txt’\n",
            "\n",
            "train_reviews.txt   100%[===================>] 435.66K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-12-28 16:23:50 (15.4 MB/s) - ‘train_reviews.txt’ saved [446118/446118]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#загружаем данные\n",
        "!wget https://raw.githubusercontent.com/named-entity/hse-nlp/master/4th_year/Project/train_aspects.txt\n",
        "!wget https://raw.githubusercontent.com/named-entity/hse-nlp/master/4th_year/Project/train_reviews.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = {}\n",
        "with open('train_reviews.txt') as f:\n",
        "  for line in f:\n",
        "    line = line.rstrip('\\r\\n').split('\\t')\n",
        "    reviews[line[0]] = line[1]"
      ],
      "metadata": {
        "id": "n-WQXy7CN8pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_aspects = pd.read_csv('train_aspects.txt', delimiter = '\\t', names = ['review_id', 'aspect', 'text', 'start', 'stop', 'sent'])\n",
        "\n",
        "aspects = defaultdict(list)\n",
        "with open('train_aspects.txt') as f:\n",
        "  for line in f:\n",
        "    line = line.rstrip('\\r\\n').split('\\t')\n",
        "    keys = ('aspect', 'text', 'start', 'stop', 'sent')\n",
        "    # ['text_id', 'category', 'mention', 'start', 'end', 'sentiment']\n",
        "    # тут можно отдельно запомнить начало и конец каждого упоминания\n",
        "    aspects[line[0]].append(dict(zip(keys, line[1:])))"
      ],
      "metadata": {
        "id": "gX-pe-loN9fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bio_df_train = bio_tagged_df(reviews=reviews,\n",
        "                             aspects=aspects)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enGrDhAU9i_s",
        "outputId": "f0588672-63f9-474b-d678-435befa88df8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [05:52<00:00,  1.24s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Наблюдается дисбаланс классов"
      ],
      "metadata": {
        "id": "gJixqjyAzHyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bio_df_train['bio_tag'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcsZRg-Y9jtp",
        "outputId": "3b4f0988-4be6-4215-ab42-30d5b7ea057a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "O             40858\n",
              "B-Food         1877\n",
              "B-Service      1246\n",
              "I-Food          959\n",
              "B-Whole         795\n",
              "B-Interior      686\n",
              "I-Service       283\n",
              "I-Interior      189\n",
              "I-Whole         187\n",
              "B-Price         134\n",
              "I-Price          30\n",
              "Name: bio_tag, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Тест"
      ],
      "metadata": {
        "id": "TGkKCVoahmg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#загружаем данные\n",
        "!wget https://raw.githubusercontent.com/named-entity/hse-nlp/master/4th_year/Project/dev_aspects.txt\n",
        "!wget https://raw.githubusercontent.com/named-entity/hse-nlp/master/4th_year/Project/dev_reviews.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yv4dhdANhsbS",
        "outputId": "b885f097-2825-4004-bc69-9f282d71d5a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-28 16:43:20--  https://raw.githubusercontent.com/named-entity/hse-nlp/master/4th_year/Project/dev_aspects.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 57508 (56K) [text/plain]\n",
            "Saving to: ‘dev_aspects.txt’\n",
            "\n",
            "\rdev_aspects.txt       0%[                    ]       0  --.-KB/s               \rdev_aspects.txt     100%[===================>]  56.16K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2022-12-28 16:43:21 (7.02 MB/s) - ‘dev_aspects.txt’ saved [57508/57508]\n",
            "\n",
            "--2022-12-28 16:43:21--  https://raw.githubusercontent.com/named-entity/hse-nlp/master/4th_year/Project/dev_reviews.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 110515 (108K) [text/plain]\n",
            "Saving to: ‘dev_reviews.txt’\n",
            "\n",
            "dev_reviews.txt     100%[===================>] 107.92K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-12-28 16:43:21 (7.73 MB/s) - ‘dev_reviews.txt’ saved [110515/110515]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = {}\n",
        "with open('dev_reviews.txt') as f:\n",
        "  for line in f:\n",
        "    line = line.rstrip('\\r\\n').split('\\t')\n",
        "    reviews[line[0]] = line[1]\n",
        "\n",
        "test_aspects = pd.read_csv('dev_aspects.txt', delimiter = '\\t', names = ['review_id', 'aspect', 'text', 'start', 'stop', 'sent'])\n",
        "\n",
        "aspects = defaultdict(list)\n",
        "with open('dev_aspects.txt') as f:\n",
        "  for line in f:\n",
        "    line = line.rstrip('\\r\\n').split('\\t')\n",
        "    keys = ('aspect', 'text', 'start', 'stop', 'sent')\n",
        "    # ['text_id', 'category', 'mention', 'start', 'end', 'sentiment']\n",
        "    # тут можно отдельно запомнить начало и конец каждого упоминания\n",
        "    aspects[line[0]].append(dict(zip(keys, line[1:])))"
      ],
      "metadata": {
        "id": "waR8AUd_hsnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bio_df_test = bio_tagged_df(reviews=reviews,\n",
        "                            aspects=aspects)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l21XjZUzhswb",
        "outputId": "875e125e-d1b0-4aff-eec5-9cb7ee96362e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 71/71 [01:25<00:00,  1.20s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CRF"
      ],
      "metadata": {
        "id": "8au5_VPHQBWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word2features(sent, i):\n",
        "    # достаёт фичи для i-го токена в предложении\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "    \n",
        "    features = {\n",
        "        'word.lower()': word.lower(),\n",
        "        'word[-3:]': word[-3:],\n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "        'postag': postag,\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        postag1 = sent[i-1][1]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "            '-1:postag': postag1,\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "        \n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        postag1 = sent[i+1][1]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "            '+1:postag': postag1,\n",
        "            '+1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "                \n",
        "    return features\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    # достаёт фичи для всех токенов в предложении\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [label for token, postag, label in sent]\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [token for token, postag, label in sent]"
      ],
      "metadata": {
        "id": "I8jYYb3-ecw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_train = defaultdict(list)\n",
        "for elem in bio_df_train.itertuples():\n",
        "  sentences_train[elem.text_id].append((elem.text, elem.pos_tag, elem.bio_tag))\n",
        "\n",
        "sentences_train = list(sentences_train.values())\n",
        "\n",
        "sentences_test = defaultdict(list)\n",
        "for elem in bio_df_test.itertuples():\n",
        "  sentences_test[elem.text_id].append((elem.text, elem.pos_tag, elem.bio_tag))\n",
        "\n",
        "sentences_test = list(sentences_test.values())"
      ],
      "metadata": {
        "id": "aqLx1Qq4Wtr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = [sent2features(s) for s in sentences_train]\n",
        "y_train = [sent2labels(s) for s in sentences_train]\n",
        "X_test = [sent2features(s) for s in sentences_test]\n",
        "y_test = [sent2labels(s) for s in sentences_test]"
      ],
      "metadata": {
        "id": "crnVXAf2XcUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs', \n",
        "    c1=0.1, \n",
        "    c2=0.1, \n",
        "    max_iterations=100, \n",
        "    all_possible_transitions=True\n",
        ")\n",
        "crf.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6OPkZ2fXzsM",
        "outputId": "36fdcaef-d343-4ca5-b5a6-e3414ce34da2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
            "  warnings.warn('From version 0.24, get_params will raise an '\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n",
              "    keep_tempfiles=None, max_iterations=100)"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = list(set([elem for e in y_train for elem in e]))\n",
        "\n",
        "y_pred = crf.predict(X_test)\n",
        "metrics.flat_f1_score(y_test, y_pred, \n",
        "                      average='weighted', labels=labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_iK9iuqY6zt",
        "outputId": "437a0616-1376-47ae-83f2-f6f973b3c2be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9887204851129531"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_labels = sorted(\n",
        "    labels, \n",
        "    key=lambda name: (name[1:], name[0])\n",
        ")\n",
        "\n",
        "print(metrics.flat_classification_report(\n",
        "    y_test, y_pred, labels=sorted_labels, digits=3\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3KgVjRcZCbH",
        "outputId": "48d8e828-bb47-4b0b-f1d0-3c0914db801b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           O      0.994     0.995     0.994     10211\n",
            "      B-Food      0.937     0.962     0.949       449\n",
            "      I-Food      0.982     0.993     0.987       273\n",
            "  B-Interior      0.976     0.938     0.957       176\n",
            "  I-Interior      0.853     1.000     0.921        29\n",
            "     B-Price      0.971     1.000     0.986        34\n",
            "     I-Price      1.000     1.000     1.000        11\n",
            "   B-Service      0.971     0.899     0.934       338\n",
            "   I-Service      0.958     0.919     0.938        74\n",
            "     B-Whole      0.926     0.941     0.933       185\n",
            "     I-Whole      1.000     0.979     0.989        48\n",
            "\n",
            "    accuracy                          0.989     11828\n",
            "   macro avg      0.961     0.966     0.963     11828\n",
            "weighted avg      0.989     0.989     0.989     11828\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:67: FutureWarning: Pass labels=['O', 'B-Food', 'I-Food', 'B-Interior', 'I-Interior', 'B-Price', 'I-Price', 'B-Service', 'I-Service', 'B-Whole', 'I-Whole'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
            "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def raw_cat_from_bio(text):\n",
        "  return text.replace('B-', '').replace('I-', '')"
      ],
      "metadata": {
        "id": "HkJ_A87Dtv0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entity_set = []\n",
        "textid = 0\n",
        "cur_start = 0\n",
        "cur_end = 0\n",
        "cur_word = ''\n",
        "cur_tag = ''\n",
        "typ = ''\n",
        "for index, row in df_predicted.iterrows():\n",
        "    if row.crf_predictions == 'O':\n",
        "        continue\n",
        "    typ, tag = row.crf_predictions.split('-')\n",
        "    if typ == 'B':\n",
        "        if cur_word:\n",
        "            entity_set.append((str(cur_text), cur_tag, cur_word, str(cur_start), str(cur_end)))\n",
        "        cur_text = row['Text ID']\n",
        "        cur_tag = tag\n",
        "        cur_word = row.Word\n",
        "        cur_start = int(row.Start)\n",
        "        cur_end = int(row.End)\n",
        "    elif typ == 'I':\n",
        "        cur_word = cur_word + ' ' * (int(row.Start) - cur_end) + row.Word\n",
        "        cur_end = int(row.End)\n",
        "entity_set.append((str(cur_text), cur_tag, cur_word, str(cur_start), str(cur_end)))\n",
        "     \n"
      ],
      "metadata": {
        "id": "XPdtVLyO7kGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#перевод в обычный формат\n",
        "\n",
        "predicted_tags = [elem for e in y_pred for elem in e]\n",
        "\n",
        "aspect_spans = []\n",
        "text_id = 0\n",
        "cur_span = ''\n",
        "start = 0\n",
        "end = 0\n",
        "tag_ = ''\n",
        "\n",
        "for elem in bio_df_test.itertuples():\n",
        "  tag = predicted_tags[elem.Index]\n",
        "  if tag != 'O':\n",
        "    \n",
        "    # продолжение последовательности\n",
        "    if tag.startswith('I-'):\n",
        "\n",
        "      spaces = ' ' * (int(elem.start) - int(end))\n",
        "      cur_span = cur_span + spaces + elem.text \n",
        "      end = elem.stop\n",
        "\n",
        "    #обновление последовательности\n",
        "    elif tag.startswith('B-'):\n",
        "\n",
        "      if cur_span:\n",
        "        aspect_spans.append((text_id, cur_span, start, end, tag_))\n",
        "\n",
        "      cur_span = elem.text\n",
        "      text_id = str(elem.text_id)\n",
        "      start = str(elem.start)\n",
        "      end = str(elem.stop)\n",
        "      tag_ = tag.replace('B-', '')\n",
        "\n",
        "aspect_spans.append((text_id, cur_span, start, end, tag_))"
      ],
      "metadata": {
        "id": "4-uUrBPv67HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aspect_spans[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHXIzlebAaWA",
        "outputId": "52e028e2-cf1a-401d-e708-7af5772dbc67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('13823', '\"аппетит\"', '7', 16, 'Whole'),\n",
              " ('13823', 'встретил', '138', '146', 'Service'),\n",
              " ('13823', 'менеджер', '147', '155', 'Service'),\n",
              " ('13823', 'девушка', '179', '186', 'Service'),\n",
              " ('13823', 'проводила к столу', '188', 205, 'Service')]"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#запись результатов выделения аспектов в файл\n",
        "\n",
        "with open('dev_aspects_spans_predicted.txt', 'w', encoding = 'utf-8') as f:\n",
        "  for elem in aspect_spans:\n",
        "    f.write('\\t'.join([str(e) for e in elem]) + '\\n')"
      ],
      "metadata": {
        "id": "u-5mN97toJ0C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}